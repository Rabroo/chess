{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Position Evaluation Model Training\n",
    "\n",
    "Train neural networks to predict Stockfish evaluation from chess positions.\n",
    "\n",
    "**Features:**\n",
    "- Downloads pre-computed evaluations from Lichess (depth 30-40+)\n",
    "- 17 model architectures to compare\n",
    "- Uses GPU for fast training\n",
    "\n",
    "**Runtime:** Select GPU: `Runtime → Change runtime type → T4 GPU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch numpy requests\n",
    "!apt-get install -qq zstd\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Lichess Evaluation Data (Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "NUM_POSITIONS = 100000  # Start with 100K, increase to 1M for better results\n",
    "MIN_DEPTH = 30          # Minimum analysis depth (30+ is very accurate)\n",
    "SKIP_MATES = True       # Skip mate positions for stable training\n",
    "\n",
    "print(f\"Will extract {NUM_POSITIONS:,} positions with depth >= {MIN_DEPTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "LICHESS_EVAL_URL = \"https://database.lichess.org/lichess_db_eval.jsonl.zst\"\n",
    "\n",
    "def stream_and_extract_positions(limit, min_depth, skip_mates=True):\n",
    "    \"\"\"\n",
    "    Stream Lichess eval database and extract positions.\n",
    "    No disk storage needed - pipes directly from URL.\n",
    "    \"\"\"\n",
    "    positions = []\n",
    "    scores = []\n",
    "    \n",
    "    print(f\"Streaming from Lichess database...\")\n",
    "    print(f\"Target: {limit:,} positions, min depth: {min_depth}\")\n",
    "    print(\"This may take 10-30 minutes depending on depth filter...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Stream: curl -> zstd decompress -> python\n",
    "    process = subprocess.Popen(\n",
    "        f'curl -sL \"{LICHESS_EVAL_URL}\" | zstd -d',\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "        text=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    processed = 0\n",
    "    last_print = 0\n",
    "    \n",
    "    for line in process.stdout:\n",
    "        if len(positions) >= limit:\n",
    "            break\n",
    "            \n",
    "        processed += 1\n",
    "        \n",
    "        # Progress update every 10K entries\n",
    "        if processed - last_print >= 10000:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = processed / elapsed\n",
    "            print(f\"\\rProcessed: {processed:,} | Found: {len(positions):,}/{limit:,} | Rate: {rate:.0f}/s\", end=\"\")\n",
    "            last_print = processed\n",
    "        \n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            \n",
    "            fen = entry.get(\"fen\")\n",
    "            evals = entry.get(\"evals\", [])\n",
    "            \n",
    "            if not fen or not evals:\n",
    "                continue\n",
    "            \n",
    "            # Get best/deepest eval\n",
    "            best_eval = max(evals, key=lambda e: e.get(\"depth\", 0))\n",
    "            depth = best_eval.get(\"depth\", 0)\n",
    "            \n",
    "            if depth < min_depth:\n",
    "                continue\n",
    "            \n",
    "            pvs = best_eval.get(\"pvs\", [])\n",
    "            if not pvs:\n",
    "                continue\n",
    "            \n",
    "            pv = pvs[0]\n",
    "            \n",
    "            # Get score\n",
    "            if \"cp\" in pv:\n",
    "                score_cp = pv[\"cp\"]\n",
    "            elif \"mate\" in pv:\n",
    "                if skip_mates:\n",
    "                    continue\n",
    "                mate_in = pv[\"mate\"]\n",
    "                score_cp = 10000 - abs(mate_in) * 10\n",
    "                if mate_in < 0:\n",
    "                    score_cp = -score_cp\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Skip extreme scores for training stability\n",
    "            if abs(score_cp) > 5000:\n",
    "                continue\n",
    "            \n",
    "            positions.append(fen)\n",
    "            scores.append(score_cp)\n",
    "            \n",
    "        except (json.JSONDecodeError, Exception):\n",
    "            continue\n",
    "    \n",
    "    process.terminate()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n\\nDone! Extracted {len(positions):,} positions in {elapsed:.1f}s\")\n",
    "    \n",
    "    return positions, scores\n",
    "\n",
    "# Extract positions\n",
    "positions, scores = stream_and_extract_positions(\n",
    "    limit=NUM_POSITIONS,\n",
    "    min_depth=MIN_DEPTH,\n",
    "    skip_mates=SKIP_MATES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick stats\n",
    "import numpy as np\n",
    "\n",
    "scores_np = np.array(scores)\n",
    "print(f\"Positions: {len(positions):,}\")\n",
    "print(f\"Score range: {scores_np.min():.0f} to {scores_np.max():.0f} centipawns\")\n",
    "print(f\"Mean score: {scores_np.mean():.1f} cp\")\n",
    "print(f\"Std dev: {scores_np.std():.1f} cp\")\n",
    "\n",
    "# Sample positions\n",
    "print(\"\\nSample positions:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {positions[i][:50]}... → {scores[i]} cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "# Piece mappings\n",
    "PIECE_TO_INDEX = {\n",
    "    'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "    'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11,\n",
    "}\n",
    "\n",
    "def fen_to_tensor(fen):\n",
    "    \"\"\"Convert FEN to 12x8x8 tensor.\"\"\"\n",
    "    board = np.zeros((12, 8, 8), dtype=np.float32)\n",
    "    piece_placement = fen.split()[0]\n",
    "    row, col = 0, 0\n",
    "    \n",
    "    for char in piece_placement:\n",
    "        if char == '/':\n",
    "            row += 1\n",
    "            col = 0\n",
    "        elif char.isdigit():\n",
    "            col += int(char)\n",
    "        elif char in PIECE_TO_INDEX:\n",
    "            if row < 8 and col < 8:\n",
    "                board[PIECE_TO_INDEX[char], row, col] = 1.0\n",
    "            col += 1\n",
    "    \n",
    "    return board\n",
    "\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, positions, scores):\n",
    "        print(\"Converting FEN to tensors...\")\n",
    "        self.boards = np.array([fen_to_tensor(fen) for fen in positions])\n",
    "        self.scores = np.array(scores, dtype=np.float32)\n",
    "        \n",
    "        # Normalize scores\n",
    "        self.score_scale = 500.0  # 5 pawns = 1.0\n",
    "        self.scores_normalized = self.scores / self.score_scale\n",
    "        print(f\"Dataset ready: {len(self)} positions\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.boards)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.boards[idx]),\n",
    "            torch.tensor(self.scores_normalized[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "# Create dataset\n",
    "dataset = ChessDataset(positions, scores)\n",
    "\n",
    "# Split: 80% train, 10% val, 10% test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Data loaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# ============= MLP Models =============\n",
    "\n",
    "class MLP_Small(nn.Module):\n",
    "    \"\"\"Small MLP - 200K params, fast baseline\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze(-1)\n",
    "\n",
    "class MLP_Large(nn.Module):\n",
    "    \"\"\"Large MLP - 1M params\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12 * 8 * 8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze(-1)\n",
    "\n",
    "# ============= CNN Models =============\n",
    "\n",
    "class CNN_Medium(nn.Module):\n",
    "    \"\"\"Medium CNN - 13M params\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(12, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x).squeeze(-1)\n",
    "\n",
    "# ============= ResNet Models =============\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "class ResNet_Small(nn.Module):\n",
    "    \"\"\"Small ResNet - 600K params, AlphaZero-style\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(12, 64, 3, padding=1)\n",
    "        self.bn_in = nn.BatchNorm2d(64)\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(4)])\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn_in(self.conv_in(x)))\n",
    "        x = self.blocks(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x).squeeze(-1)\n",
    "\n",
    "class ResNet_Medium(nn.Module):\n",
    "    \"\"\"Medium ResNet - 3M params\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(12, 128, 3, padding=1)\n",
    "        self.bn_in = nn.BatchNorm2d(128)\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(128) for _ in range(8)])\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn_in(self.conv_in(x)))\n",
    "        x = self.blocks(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x).squeeze(-1)\n",
    "\n",
    "class ResNet_Large(nn.Module):\n",
    "    \"\"\"Large ResNet - 20M params, Lc0-style\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(12, 256, 3, padding=1)\n",
    "        self.bn_in = nn.BatchNorm2d(256)\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(256) for _ in range(16)])\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn_in(self.conv_in(x)))\n",
    "        x = self.blocks(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x).squeeze(-1)\n",
    "\n",
    "# ============= Transformer =============\n",
    "\n",
    "class ChessTransformer(nn.Module):\n",
    "    \"\"\"Transformer - 5M params\"\"\"\n",
    "    def __init__(self, d_model=256, nhead=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(12, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 64, d_model) * 0.02)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc1 = nn.Linear(d_model, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = x.view(B, 12, 64).permute(0, 2, 1)\n",
    "        x = self.input_proj(x) + self.pos_encoding\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x).squeeze(-1)\n",
    "\n",
    "# ============= NNUE-style =============\n",
    "\n",
    "class NNUE_Style(nn.Module):\n",
    "    \"\"\"NNUE-style - 500K params, fast inference like Stockfish\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(12 * 64, 256)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.clamp(F.relu(self.fc1(x)), 0, 1)\n",
    "        x = torch.clamp(F.relu(self.fc2(x)), 0, 1)\n",
    "        x = torch.clamp(F.relu(self.fc3(x)), 0, 1)\n",
    "        return self.fc4(x).squeeze(-1)\n",
    "\n",
    "# Model registry\n",
    "MODELS = {\n",
    "    \"mlp_small\": MLP_Small,\n",
    "    \"mlp_large\": MLP_Large,\n",
    "    \"cnn\": CNN_Medium,\n",
    "    \"resnet_small\": ResNet_Small,\n",
    "    \"resnet_medium\": ResNet_Medium,\n",
    "    \"resnet_large\": ResNet_Large,\n",
    "    \"transformer\": ChessTransformer,\n",
    "    \"nnue\": NNUE_Style,\n",
    "}\n",
    "\n",
    "print(f\"Available models: {list(MODELS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, device, score_scale=500.0):\n",
    "    \"\"\"Train a model and return metrics.\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_mae = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_mae += torch.abs(outputs - batch_y).mean().item() * score_scale\n",
    "        val_loss /= len(val_loader)\n",
    "        val_mae /= len(val_loader)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict().copy()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | MAE: {val_mae:.1f} cp\")\n",
    "    \n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, device, score_scale=500.0):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            start = time.perf_counter()\n",
    "            outputs = model(batch_x)\n",
    "            inference_times.append((time.perf_counter() - start) / len(batch_x))\n",
    "            predictions.extend((outputs.cpu().numpy() * score_scale).tolist())\n",
    "            actuals.extend((batch_y.numpy() * score_scale).tolist())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals) ** 2))\n",
    "    correlation = np.corrcoef(predictions, actuals)[0, 1]\n",
    "    throughput = 1000 / (np.mean(inference_times) * 1000)\n",
    "    \n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "        \"correlation\": correlation,\n",
    "        \"throughput\": throughput,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model\n",
    "MODEL_NAME = \"resnet_medium\"  # Options: mlp_small, mlp_large, cnn, resnet_small, resnet_medium, resnet_large, transformer, nnue\n",
    "EPOCHS = 30\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = MODELS[MODEL_NAME]().to(device)\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Train\n",
    "print(f\"\\nTraining for {EPOCHS} epochs...\")\n",
    "print(\"-\" * 50)\n",
    "model = train_model(model, train_loader, val_loader, EPOCHS, device, dataset.score_scale)\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluate_model(model, test_loader, device, dataset.score_scale)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RESULTS: {MODEL_NAME}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"MAE: {metrics['mae']:.1f} centipawns ({metrics['mae']/100:.2f} pawns)\")\n",
    "print(f\"RMSE: {metrics['rmse']:.1f} centipawns\")\n",
    "print(f\"Correlation: {metrics['correlation']:.4f}\")\n",
    "print(f\"Throughput: {metrics['throughput']:.0f} positions/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "EPOCHS = 20  # Fewer epochs for comparison\n",
    "results = []\n",
    "\n",
    "for name, model_class in MODELS.items():\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    model = model_class().to(device)\n",
    "    params = count_parameters(model)\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    \n",
    "    model = train_model(model, train_loader, val_loader, EPOCHS, device, dataset.score_scale)\n",
    "    metrics = evaluate_model(model, test_loader, device, dataset.score_scale)\n",
    "    \n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"params\": params,\n",
    "        **metrics\n",
    "    })\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f\"best_{name}.pth\")\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Model':<20} {'Params':>10} {'MAE (cp)':>10} {'Corr':>8} {'Speed':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in sorted(results, key=lambda x: x['mae']):\n",
    "    print(f\"{r['model']:<20} {r['params']:>10,} {r['mae']:>10.1f} {r['correlation']:>8.4f} {r['throughput']:>10.0f}/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Save best model\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "save_dir = \"/content/drive/MyDrive/chess_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Find best model from comparison\n",
    "if results:\n",
    "    best = min(results, key=lambda x: x['mae'])\n",
    "    print(f\"Best model: {best['model']} (MAE: {best['mae']:.1f} cp)\")\n",
    "    \n",
    "    src = f\"best_{best['model']}.pth\"\n",
    "    dst = f\"{save_dir}/best_{best['model']}.pth\"\n",
    "    shutil.copy(src, dst)\n",
    "    print(f\"Saved to: {dst}\")\n",
    "else:\n",
    "    # Save single model\n",
    "    src = f\"best_{MODEL_NAME}.pth\"\n",
    "    dst = f\"{save_dir}/best_{MODEL_NAME}.pth\"\n",
    "    shutil.copy(src, dst)\n",
    "    print(f\"Saved to: {dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on some positions\n",
    "test_fens = [\n",
    "    \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\",  # Starting position\n",
    "    \"rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq e3 0 1\",  # After 1.e4\n",
    "    \"r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4\",  # Italian Game\n",
    "    \"8/8/8/8/8/5K2/4Q3/7k w - - 0 1\",  # King + Queen vs King (winning)\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "print(\"Position Predictions:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for fen in test_fens:\n",
    "    tensor = torch.from_numpy(fen_to_tensor(fen)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(tensor).item() * dataset.score_scale\n",
    "    print(f\"{fen[:40]}...\")\n",
    "    print(f\"  Predicted: {pred:+.0f} cp ({pred/100:+.2f} pawns)\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
