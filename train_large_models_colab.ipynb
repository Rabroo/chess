{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Large Chess Evaluation Models\n",
    "\n",
    "Train resnet_large, transformer_large, cnn_deep on Google Colab GPU.\n",
    "\n",
    "**Setup:** Runtime → Change runtime type → T4 GPU (or A100 for faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your chess_quality.tsv file\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "print(f\"Uploaded: {list(uploaded.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEN to tensor conversion\n",
    "PIECE_MAP = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "             'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "\n",
    "def fen_to_tensor(fen):\n",
    "    board = np.zeros((12, 8, 8), dtype=np.float32)\n",
    "    parts = fen.split()\n",
    "    rows = parts[0].split('/')\n",
    "    for row_idx, row in enumerate(rows):\n",
    "        col_idx = 0\n",
    "        for char in row:\n",
    "            if char.isdigit():\n",
    "                col_idx += int(char)\n",
    "            else:\n",
    "                piece_idx = PIECE_MAP.get(char)\n",
    "                if piece_idx is not None:\n",
    "                    board[piece_idx, row_idx, col_idx] = 1.0\n",
    "                col_idx += 1\n",
    "    return board\n",
    "\n",
    "def parse_score(score_str):\n",
    "    try:\n",
    "        return float(score_str.strip()) * 100\n",
    "    except:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, tsv_path, max_score=15000):\n",
    "        self.positions = []\n",
    "        self.scores = []\n",
    "        \n",
    "        print(f\"Loading {tsv_path}...\")\n",
    "        with open(tsv_path) as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if i % 100000 == 0:\n",
    "                print(f\"\\rProcessing: {i:,}/{len(lines):,}\", end=\"\", flush=True)\n",
    "            \n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            \n",
    "            fen, score_str = parts[0], parts[1]\n",
    "            score = parse_score(score_str)\n",
    "            \n",
    "            if max_score > 0 and abs(score) > max_score:\n",
    "                continue\n",
    "            \n",
    "            self.positions.append(fen_to_tensor(fen))\n",
    "            self.scores.append(score)\n",
    "        \n",
    "        print(f\"\\nLoaded {len(self.positions):,} positions\")\n",
    "        \n",
    "        self.positions = np.array(self.positions)\n",
    "        self.scores = np.array(self.scores, dtype=np.float32)\n",
    "        self.score_scale = 500.0\n",
    "        self.scores_normalized = self.scores / self.score_scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.positions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.from_numpy(self.positions[idx]),\n",
    "                torch.tensor(self.scores_normalized[idx], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return torch.relu(x + residual)\n",
    "\n",
    "class ResNetLarge(nn.Module):\n",
    "    \"\"\"Large ResNet (16 blocks, 256ch) - 20M params, Lc0-style\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(12, 256, 3, padding=1)\n",
    "        self.bn_in = nn.BatchNorm2d(256)\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(256) for _ in range(16)])\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn_in(self.conv_in(x)))\n",
    "        x = self.blocks(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x).squeeze(-1)\n",
    "\n",
    "class TransformerLarge(nn.Module):\n",
    "    \"\"\"Large Transformer (8 layers, 512dim) - 20M params\"\"\"\n",
    "    def __init__(self, d_model=512, nhead=8, num_layers=8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.input_proj = nn.Linear(12, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(64, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, d_model * 4, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, 12, 64).permute(0, 2, 1)\n",
    "        x = self.input_proj(x) + self.pos_embedding\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "class CNNDeep(nn.Module):\n",
    "    \"\"\"Deep CNN (5 layers) - 25M params\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(12, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.fc1 = nn.Linear(512 * 8 * 8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = torch.relu(self.bn5(self.conv5(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        return self.fc2(x).squeeze(-1)\n",
    "\n",
    "class MLPLarge(nn.Module):\n",
    "    \"\"\"Large MLP (1024-512-256) - 1M params\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12 * 8 * 8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze(-1)\n",
    "\n",
    "MODELS = {\n",
    "    'resnet_large': ResNetLarge,\n",
    "    'transformer_large': TransformerLarge,\n",
    "    'cnn_deep': CNNDeep,\n",
    "    'mlp_large': MLPLarge,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs, device, score_scale, model_name):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = f'best_{model_name}.pth'\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            if batch_idx % 500 == 0:\n",
    "                print(f\"\\rEpoch {epoch+1}/{epochs} | Batch {batch_idx}/{len(train_loader)}\", end=\"\", flush=True)\n",
    "            \n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_mae = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_mae += torch.abs(outputs - batch_y).mean().item() * score_scale\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_mae /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MAE: {val_mae:.1f} cp\")\n",
    "    \n",
    "    return best_val_loss, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset = ChessDataset('chess_quality.tsv')\n",
    "\n",
    "# Split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=2)\n",
    "\n",
    "print(f\"Train: {len(train_dataset):,}, Val: {len(val_dataset):,}, Test: {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "EPOCHS = 50\n",
    "MODELS_TO_TRAIN = ['resnet_large', 'transformer_large', 'cnn_deep', 'mlp_large']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all large models\n",
    "results = {}\n",
    "\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = MODELS[model_name]().to(device)\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    \n",
    "    val_loss, model_path = train_model(\n",
    "        model, train_loader, val_loader, EPOCHS, device,\n",
    "        dataset.score_scale, model_name\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    test_mae = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            test_mae += torch.abs(outputs - batch_y).mean().item() * dataset.score_scale\n",
    "    test_mae /= len(test_loader)\n",
    "    \n",
    "    results[model_name] = {'params': params, 'mae': test_mae}\n",
    "    print(f\"\\n{model_name} Test MAE: {test_mae:.1f} cp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Params':>12} {'Test MAE':>10}\")\n",
    "print(\"-\"*60)\n",
    "for name, r in sorted(results.items(), key=lambda x: x[1]['mae']):\n",
    "    print(f\"{name:<20} {r['params']:>12,} {r['mae']:>10.1f} cp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained models\n",
    "from google.colab import files\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    files.download(f'best_{model_name}.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
